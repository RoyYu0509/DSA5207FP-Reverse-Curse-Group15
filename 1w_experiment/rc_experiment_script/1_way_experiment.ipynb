{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must run\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85872f2b",
   "metadata": {},
   "source": [
    "# One Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b77906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/nk/7m8bs7zj68dcqhl1g64gcwh80000gn/T/ipykernel_63179/3253010523.py\", line 11, in <module>\n",
      "    import torch\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment start for meta-llama/Llama-3.2-1B ***\n",
      "\n",
      "DatasetDict({\n",
      "    1wtrain: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 1800\n",
      "    })\n",
      "    1wvalidation: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c5f2570e4e4c2ba63efac962e4df6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    1wtrain: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1800\n",
      "    })\n",
      "    1wvalidation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:550: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 13,357,056 || all params: 1,249,171,456 || trainable%: 1.0693\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 900/900 [11:26<00:00,  1.31batch/s]\n",
      "Evaluating:   0%|          | 0/1 [00:00<?, ?batch/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/transformers/pytorch_utils.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n",
      "Evaluating: 100%|██████████| 1/1 [00:07<00:00,  7.81s/batch]\n",
      "/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | Train Loss: 0.6189 | Val Error Rate: 50.00%\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 900/900 [11:01<00:00,  1.36batch/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.15s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02/20 | Train Loss: 0.0229 | Val Error Rate: 0.79%\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 900/900 [10:59<00:00,  1.36batch/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.11s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03/20 | Train Loss: 0.0142 | Val Error Rate: 16.67%\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 900/900 [10:38<00:00,  1.41batch/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.11s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04/20 | Train Loss: 0.0158 | Val Error Rate: 0.79%\n",
      "\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 900/900 [10:28<00:00,  1.43batch/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.09s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05/20 | Train Loss: 0.0102 | Val Error Rate: 0.79%\n",
      "Early stopping triggered (no improvement in error‑rate).\n",
      "DatasetDict({\n",
      "    1wp2d: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    1wp2d: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "{'1wp2d_loader': <torch.utils.data.dataloader.DataLoader object at 0x3557de260>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 150/150 [01:58<00:00,  1.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Coverage Accuracy: 86.65% (259.9427844793338/300)\n",
      "DataFrame successfully saved to experiment_rslt/meta-llama/Llama-3.2-1B/1wp2d_results.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4724195c7ba54a12a28b9a49e964f738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating 1wd2p split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    1wd2p: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277a407c0a964c7cbdac4ae892c4a5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    1wd2p: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "{'1wd2p_loader': <torch.utils.data.dataloader.DataLoader object at 0x35c50f4c0>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 150/150 [01:59<00:00,  1.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Coverage Accuracy: 100.00% (300.0/300)\n",
      "DataFrame successfully saved to experiment_rslt/meta-llama/Llama-3.2-1B/1wd2p_results.csv\n",
      "\n",
      "*** Experiment start for allenai/OLMo-2-0425-1B-Instruct ***\n",
      "\n",
      "DatasetDict({\n",
      "    1wtrain: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 1800\n",
      "    })\n",
      "    1wvalidation: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe960c412ff404c807a0de9ec2ae5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e752dd1487342a0bed8aec0ca11c81f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    1wtrain: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1800\n",
      "    })\n",
      "    1wvalidation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "trainable params: 13,697,024 || all params: 1,498,613,760 || trainable%: 0.9140\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 900/900 [10:54<00:00,  1.38batch/s]\n",
      "Evaluating:   0%|          | 0/1 [00:00<?, ?batch/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.48s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | Train Loss: 0.6655 | Val Error Rate: 0.79%\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 900/900 [10:54<00:00,  1.37batch/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02/20 | Train Loss: 0.0241 | Val Error Rate: 100.00%\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 900/900 [10:54<00:00,  1.38batch/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.10s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03/20 | Train Loss: 0.0122 | Val Error Rate: 0.79%\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 900/900 [10:54<00:00,  1.37batch/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04/20 | Train Loss: 0.0095 | Val Error Rate: 16.67%\n",
      "Early stopping triggered (no improvement in error‑rate).\n",
      "DatasetDict({\n",
      "    1wp2d: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    1wp2d: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "{'1wp2d_loader': <torch.utils.data.dataloader.DataLoader object at 0x35c5fc820>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 150/150 [01:57<00:00,  1.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Coverage Accuracy: 85.62% (256.8631904622398/300)\n",
      "DataFrame successfully saved to experiment_rslt/allenai/OLMo-2-0425-1B-Instruct/1wp2d_results.csv\n",
      "DatasetDict({\n",
      "    1wd2p: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67cf331e3042429bafe530760a22980c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    1wd2p: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "{'1wd2p_loader': <torch.utils.data.dataloader.DataLoader object at 0x174682e00>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 150/150 [01:58<00:00,  1.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Coverage Accuracy: 99.67% (299.0/300)\n",
      "DataFrame successfully saved to experiment_rslt/allenai/OLMo-2-0425-1B-Instruct/1wd2p_results.csv\n",
      "\n",
      "*** Experiment start for TinyLlama/TinyLlama-1.1B-Chat-v0.1 ***\n",
      "\n",
      "DatasetDict({\n",
      "    1wtrain: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 1800\n",
      "    })\n",
      "    1wvalidation: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7aef2dc9994b23bdbdd7d9dc65cc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c85a6c8a77041d8beabf98c8728f166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    1wtrain: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1800\n",
      "    })\n",
      "    1wvalidation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "trainable params: 13,160,464 || all params: 1,113,212,944 || trainable%: 1.1822\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 900/900 [09:43<00:00,  1.54batch/s]\n",
      "Evaluating:   0%|          | 0/1 [00:00<?, ?batch/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.53s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | Train Loss: 0.8911 | Val Error Rate: 50.00%\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 900/900 [09:43<00:00,  1.54batch/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.12s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02/20 | Train Loss: 0.0258 | Val Error Rate: 0.00%\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 900/900 [09:43<00:00,  1.54batch/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.13s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03/20 | Train Loss: 0.0136 | Val Error Rate: 50.00%\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 900/900 [09:49<00:00,  1.53batch/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04/20 | Train Loss: 0.0105 | Val Error Rate: 0.00%\n",
      "\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 900/900 [10:01<00:00,  1.50batch/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05/20 | Train Loss: 0.0047 | Val Error Rate: 0.00%\n",
      "Early stopping triggered (no improvement in error‑rate).\n",
      "DatasetDict({\n",
      "    1wp2d: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    1wp2d: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "{'1wp2d_loader': <torch.utils.data.dataloader.DataLoader object at 0x32cba2800>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 150/150 [02:11<00:00,  1.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Coverage Accuracy: 86.58% (259.73388888730136/300)\n",
      "DataFrame successfully saved to experiment_rslt/TinyLlama/TinyLlama-1.1B-Chat-v0.1/1wp2d_results.csv\n",
      "DatasetDict({\n",
      "    1wd2p: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1c9afb35694733a978ef73f683da0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    1wd2p: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "{'1wd2p_loader': <torch.utils.data.dataloader.DataLoader object at 0x174682e00>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 150/150 [02:13<00:00,  1.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Coverage Accuracy: 99.67% (299.0/300)\n",
      "DataFrame successfully saved to experiment_rslt/TinyLlama/TinyLlama-1.1B-Chat-v0.1/1wd2p_results.csv\n",
      "\n",
      "*** Experiment start for Qwen/Qwen3-0.6B ***\n",
      "\n",
      "DatasetDict({\n",
      "    1wtrain: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 1800\n",
      "    })\n",
      "    1wvalidation: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6beb09d8f86a4dd0ac1fb4a9f1765114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    1wtrain: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1800\n",
      "    })\n",
      "    1wvalidation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "trainable params: 12,539,904 || all params: 608,589,824 || trainable%: 2.0605\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 900/900 [08:28<00:00,  1.77batch/s]\n",
      "Evaluating:   0%|          | 0/1 [00:00<?, ?batch/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Evaluating: 100%|██████████| 1/1 [00:02<00:00,  2.40s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | Train Loss: 0.7603 | Val Error Rate: 50.00%\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 900/900 [08:28<00:00,  1.77batch/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.54s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02/20 | Train Loss: 0.0337 | Val Error Rate: 50.00%\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 900/900 [08:27<00:00,  1.77batch/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.54s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03/20 | Train Loss: 0.0174 | Val Error Rate: 50.00%\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 900/900 [08:28<00:00,  1.77batch/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.56s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04/20 | Train Loss: 0.0197 | Val Error Rate: 50.00%\n",
      "Early stopping triggered (no improvement in error‑rate).\n",
      "DatasetDict({\n",
      "    1wp2d: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30736bd5d4314504bbab09cd1b4bf712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    1wp2d: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "{'1wp2d_loader': <torch.utils.data.dataloader.DataLoader object at 0x32cac2890>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 150/150 [03:13<00:00,  1.29s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Coverage Accuracy: 77.82% (233.46664988595785/300)\n",
      "DataFrame successfully saved to experiment_rslt/Qwen/Qwen3-0.6B/1wp2d_results.csv\n",
      "DatasetDict({\n",
      "    1wd2p: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150bc2ee108d43ecbc6dfaa2a498668f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    1wd2p: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "{'1wd2p_loader': <torch.utils.data.dataloader.DataLoader object at 0x3ce7d3010>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 150/150 [03:13<00:00,  1.29s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Coverage Accuracy: 100.00% (300.0/300)\n",
      "DataFrame successfully saved to experiment_rslt/Qwen/Qwen3-0.6B/1wd2p_results.csv\n"
     ]
    }
   ],
   "source": [
    "# nvidia-smi\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Third-party\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "import matplotlib.pyplot as plt\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel,\n",
    ")\n",
    "\n",
    "\n",
    "# Local package imports\n",
    "from rc_experiment.data_loading import raw_2_llm_data, torch_data_loader\n",
    "from rc_experiment.model_loading import quanti_lora_md\n",
    "from rc_experiment.training import casual_llm_train, plot_losses\n",
    "from rc_experiment.eval import rc_eval\n",
    "\n",
    "login(token=\"YOUR_TOKEN\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if hasattr(torch, 'mps') and torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "########################################################\n",
    "# Define Experiment Configuration\n",
    "########################################################\n",
    "## Original RC Data files paths\n",
    "# TRAINING_FILES = {\n",
    "#     \"train\": \"pipeline_test_data/all_prompts_train.jsonl\",\n",
    "#     \"validation\": \"pipeline_test_data/validation_prompts.jsonl\",\n",
    "# }\n",
    "\n",
    "# TEST_FILES = {\n",
    "#     \"p2d\": \"pipeline_test_data/p2d_prompts_test.jsonl\",\n",
    "#     \"d2p\": \"pipeline_test_data/d2p_prompts_test.jsonl\"\n",
    "# }\n",
    "\n",
    "\n",
    "# #  Complex RC Data files paths\n",
    "# TRAINING_FILES = {\n",
    "#     \"train\": \"pipeline_test_data/train.jsonl\",\n",
    "#     \"validation\": \"pipeline_test_data/final_augmented_without_added.jsonl\",\n",
    "# }\n",
    "\n",
    "# TEST_FILES = {\"test\": \"pipeline_test_data/final_augmented_without_added.jsonl\"}\n",
    "\n",
    "\n",
    "# # Spatial RC Data Path\n",
    "# TRAINING_FILES = {\n",
    "#     \"train\": \"pipeline_test_data/a2b_prompts_train.jsonl\",\n",
    "#     \"validation\": \"pipeline_test_data/validation_prompts_a2b.jsonl\",\n",
    "# }\n",
    "\n",
    "# TEST_FILES = {\n",
    "#     \"test\": \"pipeline_test_data/validation_prompts_a2b.jsonl\",\n",
    "#     }\n",
    "\n",
    "\n",
    "# one-way data path\n",
    "TRAINING_FILES = {\n",
    "    \"1wtrain\": \"pipeline_test_data/one_way_training.jsonl\",\n",
    "    \"1wvalidation\": \"pipeline_test_data/validation.jsonl\",\n",
    "}\n",
    "train_file_name = \"1wtrain\"\n",
    "\n",
    "TEST_FILES = {\n",
    "    \"1wp2d\": \"pipeline_test_data/p2d_prompts_test.jsonl\",\n",
    "    \"1wd2p\": \"pipeline_test_data/d2p_prompts_test.jsonl\",\n",
    "}\n",
    "train_loader_name = \"1wtrain_loader\"\n",
    "val_loader_name = \"1wvalidation_loader\"\n",
    "\n",
    "# # two-way data path\n",
    "# TRAINING_FILES = {\n",
    "#     \"2wtrain\": \"pipeline_test_data/all_prompts_train.jsonl\",\n",
    "#     \"2wvalidation\": \"pipeline_test_data/validation.jsonl\",\n",
    "# }\n",
    "# train_file_name = \"2wtrain\"\n",
    "\n",
    "\n",
    "# TEST_FILES = {\n",
    "#     \"2wp2d\": \"pipeline_test_data/p2d_prompts_test.jsonl\",\n",
    "#     \"2wd2p\": \"pipeline_test_data/d2p_prompts_test.jsonl\"}\n",
    "# train_loader_name = \"2wtrain_loader\"\n",
    "# val_loader_name = \"2wvalidation_loader\"\n",
    "\n",
    "# Choose a small causal model from Hugging Face (for example, LLaMA-2 7B or OPT 125M)\n",
    "# TinyLlama has the same architecture as the Llama 2\n",
    "\n",
    "MODELS = [\n",
    "        # \"Qwen/Qwen3-1.7B\",\n",
    "        \"meta-llama/Llama-3.2-1B\",\n",
    "        \"allenai/OLMo-2-0425-1B-Instruct\",\n",
    "        \"TinyLlama/TinyLlama-1.1B-Chat-v0.1\",\n",
    "        \"Qwen/Qwen3-0.6B\",\n",
    "        ]\n",
    "BEST_MODEL_DIR = []  # Wait to receieve\n",
    "\n",
    "# Define max sequence lengths for prompt and completion\n",
    "MAX_INPUT_LENGTH = 256    # maximum tokens for the prompt\n",
    "MAX_TARGET_LENGTH = 20    # maximum tokens for the completion/response\n",
    "TOTAL_MAX_LENGTH = MAX_INPUT_LENGTH + MAX_TARGET_LENGTH\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_CONFIG_KWARGS = {\n",
    "    \"r\": 16,               # LoRA rank\n",
    "    \"lora_alpha\": 16,       # LoRA scaling factor\n",
    "    \"lora_dropout\": 0.05,   # LoRA dropout\n",
    "    \"bias\": \"none\",         # Bias handling\n",
    "    \"task_type\": \"CAUSAL_LM\" # Task type\n",
    "}\n",
    "\n",
    "# Training config\n",
    "BATCH_SIZE = 2\n",
    "TRAIN_PORTION_RATE = 1\n",
    "NUM_EPOCHS = 20  # you can adjust the number of fine-tuning epochs\n",
    "PATIENCE = 3    # early stopping PATIENCE\n",
    "MIN_DELTA = 0.01 # minimum change in val loss to qualify as an improvement\n",
    "\n",
    "# Define a global instruction prompt (can be multi-line or structured as needed)\n",
    "INSTRUCTION_PROMPT = \"You are a knowledgeable assistant skilled at factual recall. When given a person's name, you can return the description of that person. When given a description, you can return the name of the person that fit the description.\"\n",
    "\n",
    "# Loop all the model names to conduct experiments\n",
    "for k, model_name in enumerate(MODELS):\n",
    "    print(\"\")\n",
    "    print(f\"*** Experiment start for {model_name} ***\")\n",
    "    print(\"\")\n",
    "    ########################################################\n",
    "    # Data Loading & Pre-processing & Tokenization\n",
    "    ########################################################       \n",
    "    # Preprocess the training data with the instruction (if INSTRUCTION_PROMPT is None or \"\", no instruction will be applied)\n",
    "    tokenized_datasets, tokenizer, device = raw_2_llm_data(TRAINING_FILES, model_name, \n",
    "                                                        MAX_INPUT_LENGTH, MAX_TARGET_LENGTH, \n",
    "                                                        instruction=INSTRUCTION_PROMPT)\n",
    "\n",
    "    ########################################################\n",
    "    # Set Up Pytroch Data Loader\n",
    "    ########################################################\n",
    "    # Obtian the DataLoader dictionary\n",
    "    loader_dict = torch_data_loader(tokenized_datasets, train_file_name, batch_size=BATCH_SIZE, train_portion_rate=TRAIN_PORTION_RATE)\n",
    "    # Unpack the loader\n",
    "    train_loader = loader_dict[train_loader_name]\n",
    "    val_loader = loader_dict[val_loader_name]\n",
    "\n",
    "    ########################################################\n",
    "    # Load LoRA Model\n",
    "    ########################################################\n",
    "    # load the quantized lora model\n",
    "    model = quanti_lora_md(LORA_CONFIG_KWARGS, model_name)\n",
    "    # move the model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    ########################################################\n",
    "    # Training (Finetuning) + Save the best model\n",
    "    ########################################################\n",
    "    # Define optimizer (AdamW) to update only trainable params (LoRA adapters)\n",
    "    learning_rate = 5e-5\n",
    "    optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=learning_rate)\n",
    "\n",
    "    # Save the best model's config\n",
    "    saving_dir, train_losses, val_losses, val_accuracies = casual_llm_train(model_name, model, tokenizer, optimizer, train_loader, val_loader, device,\n",
    "                                                                            MAX_TARGET_LENGTH, NUM_EPOCHS, PATIENCE, MIN_DELTA)\n",
    "    BEST_MODEL_DIR.append(saving_dir)\n",
    "\n",
    "    plot_losses(train_losses, val_accuracies, model_name=model_name)\n",
    "    \n",
    "    \"\"\"\n",
    "    ########################################################\n",
    "    # Load in tuned model (Optional)\n",
    "    ########################################################\n",
    "    \n",
    "    # Load in model config\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    lora_weights_path = BEST_MODEL_DIR[i]\n",
    "    model = PeftModel.from_pretrained(base_model, lora_weights_path)\n",
    "    model.to(device)\n",
    "    \"\"\"\n",
    "    \n",
    "    ########################################################\n",
    "    # Evaluation on all test sets\n",
    "    ########################################################\n",
    "    for test_name, path in TEST_FILES.items():\n",
    "        test_path = {test_name: path}\n",
    "\n",
    "        test_datasets, _, _ = raw_2_llm_data(test_path, model_name, \n",
    "                                     MAX_INPUT_LENGTH, MAX_TARGET_LENGTH, \n",
    "                                     instruction=INSTRUCTION_PROMPT)\n",
    "        # Obtian the DataLoader dictionary\n",
    "        test_loader_dict = torch_data_loader(test_datasets, \"NO_TEST_SET\", batch_size=2)\n",
    "        \n",
    "        print(test_loader_dict)\n",
    "        # Get the data loader\n",
    "        test_loader = test_loader_dict[f\"{test_name}_loader\"]\n",
    "\n",
    "        pred_rslt_df = rc_eval(test_loader, model, tokenizer, device, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH, INSTRUCTION_PROMPT)\n",
    "\n",
    "        # save the data frame\n",
    "        folder_path = f\"experiment_rslt/{model_name}\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        save_path = os.path.join(folder_path, f\"{test_name}_results.csv\")\n",
    "        pred_rslt_df.to_csv(save_path, index=False)\n",
    "\n",
    "        print(f\"DataFrame successfully saved to {save_path}\")\n",
    "\n",
    "    ########################################################\n",
    "    # Clear the current model\n",
    "    ########################################################\n",
    "\n",
    "    def clear():\n",
    "        global model, tokenizer\n",
    "        del model\n",
    "        del tokenizer\n",
    "        gc.collect()\n",
    "\n",
    "        # Conditionally clear GPU caches\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "        elif torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "    clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a00974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
